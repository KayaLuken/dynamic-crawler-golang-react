# Dynamic Web Crawler - Go & React

A comprehensive web crawler application with a Next.js frontend and Go backend. Features persistent storage, dashboard analytics, and bulk operations.

## âœ¨ Features

### Core Functionality
- **URL Crawling**: Analyze websites for HTML version, headings, links, and login forms
- **Persistent Storage**: SQLite database with automatic migration and unique URL constraints
- **Dashboard Analytics**: View crawl history with sorting, filtering, and pagination
- **Detailed Reports**: Individual crawl results with charts and broken link analysis
- **Bulk Operations**: Select multiple entries for bulk delete or re-analysis

### Technical Features
- **Robust Error Handling**: Graceful handling of timeouts and inaccessible links
- **Upsert Logic**: Automatic update of existing URLs to prevent duplicates
- **Responsive UI**: Modern, mobile-friendly interface with TailwindCSS
- **Real-time Updates**: Live data refresh and background processing
- **Advanced Filtering**: Search and filter by multiple criteria

## ğŸš€ Quick Start

### Option 1: Use the Batch Script (Recommended)
```cmd
start-all.bat
```

### Option 2: Use PowerShell Script
```powershell
# Start both services
.\start-services.ps1

# Start backend only
.\start-services.ps1 -Service backend

# Start frontend only
.\start-services.ps1 -Service frontend
```

### Option 3: Manual Setup
```cmd
# Terminal 1 - Backend
cd backend
go build -o crawler.exe main.go
.\crawler.exe

# Terminal 2 - Frontend
cd frontend
npm install
npm run dev
```

## ğŸ“Š Usage

1. **Access the Application**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8080

2. **Crawl a Website**
   - Navigate to the crawl page
   - Enter a URL (e.g., https://example.com)
   - Click "Analyze Website"

3. **View Results**
   - Dashboard shows all crawl history
   - Click on any row to view detailed analytics
   - Use search and filters to find specific results

4. **Bulk Operations**
   - Select multiple entries using checkboxes
   - Use "Delete Selected" to remove entries
   - Use "Re-run Analysis" to refresh data for selected URLs

## ğŸ› ï¸ API Endpoints

### Crawl Operations
- `POST /crawl` - Crawl a single URL
- `GET /crawl/history` - Get all crawl results

### Bulk Operations
- `DELETE /crawl/bulk` - Delete multiple records
- `POST /crawl/bulk/rerun` - Re-analyze multiple URLs

## ğŸ—ï¸ Project Structure

```
dynamic-crawler-golang-react/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.go              # Entry point and routing
â”‚   â”œâ”€â”€ handler/
â”‚   â”‚   â””â”€â”€ handler.go       # HTTP handlers and crawl logic
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â””â”€â”€ crawl_service.go # Business logic and database operations
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ crawl.go         # Data models and structs
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ db.go            # Database connection and migration
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ handler_test.go  # Unit tests
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/app/
â”‚       â”œâ”€â”€ page.tsx         # Home page with crawl form
â”‚       â”œâ”€â”€ dashboard/       # Dashboard and analytics
â”‚       â”‚   â”œâ”€â”€ page.tsx
â”‚       â”‚   â”œâ”€â”€ [id]/
â”‚       â”‚   â”‚   â””â”€â”€ page.tsx # Detailed view
â”‚       â”‚   â””â”€â”€ components/
â”‚       â”‚       â”œâ”€â”€ CrawlResultsTable.tsx
â”‚       â”‚       â”œâ”€â”€ ColumnFilters.tsx
â”‚       â”‚       â””â”€â”€ LoadingComponents.tsx
â”‚       â””â”€â”€ components/
â”‚           â””â”€â”€ Nav.tsx      # Navigation component
â”œâ”€â”€ start-all.bat           # Windows batch script
â”œâ”€â”€ start-services.ps1      # PowerShell script
â””â”€â”€ README.md               # This file
```

## ğŸ“ˆ Dashboard Features

### Results Table
- **Sortable Columns**: Click headers to sort by any field
- **Global Search**: Search across all columns simultaneously
- **Column Filters**: Filter by specific criteria
- **Pagination**: Navigate through large datasets
- **Clickable Rows**: Click to view detailed analysis

### Bulk Actions
- **Checkbox Selection**: Select individual or all items
- **Bulk Delete**: Remove multiple entries at once
- **Bulk Re-analysis**: Refresh data for selected URLs
- **Selection Counter**: Shows number of selected items

### Detailed View
- **Visual Charts**: Pie and bar charts for link distribution
- **Broken Links**: List of inaccessible URLs with error details
- **SEO Metrics**: Heading structure analysis
- **Metadata**: HTML version, title, and crawl timestamp

## ğŸ”§ Development

### Backend Dependencies
- Go 1.21+
- Gin web framework
- GORM for database operations
- GoQuery for HTML parsing
- SQLite driver

### Frontend Dependencies
- Next.js 14+
- React 18+
- TailwindCSS
- TanStack Table
- Recharts for visualizations
- Lucide React icons

### Database Schema
```sql
CREATE TABLE crawl_records (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT UNIQUE NOT NULL,
    html_version TEXT,
    title TEXT,
    headings TEXT,
    internal_links INTEGER,
    external_links INTEGER,
    inaccessible_links INTEGER,
    broken_links TEXT,
    has_login_form BOOLEAN,
    crawled_at DATETIME
);
```

## ğŸ› Troubleshooting

### Common Issues
1. **Port Already in Use**: Ensure ports 3000 and 8080 are available
2. **Database Locked**: Close any existing connections before restarting
3. **CORS Errors**: Backend includes CORS middleware for localhost:3000
4. **Build Failures**: Check Go version and dependencies

### Logs and Debugging
- Backend logs are displayed in the console
- Frontend errors appear in browser console
- Database file: `backend/crawl_data.db`

## ğŸ“ License

This project is open source and available under the MIT License.